# Скрипт для пакетного перевода текста RU-EN в CSV-файлах с помощью AI/нейросети

Этот скрипт предназначен для автоматического перевода русскоязычного текста на английский язык внутри больших CSV-файлов. Он оптимизирован для работы с файлами объемом в сотни мегабайт и миллионы строк, эффективно используя ресурсы графического процессора (GPU) для ускорения процесса.

## Основные возможности

* **Обработка больших файлов:** Скрипт читает исходный файл по частям (чанками), что позволяет обрабатывать файлы, превышающие объем оперативной памяти.
* **Ускорение на GPU:** Использует библиотеку PyTorch и Hugging Face Transformers для выполнения вычислений на видеокарте NVIDIA (CUDA), что многократно ускоряет перевод.
* **Пакетная обработка (Batching):** Для максимальной эффективности GPU, скрипт собирает уникальные фразы и переводит их большими пакетами.
* **Интеллектуальное извлечение текста:** С помощью регулярных выражений находит и переводит только русскоязычные фрагменты, не затрагивая код, разметку и другие символы в строках.
* **Надежная запись результата:** Использует оптимизированные методы для записи данных, предотвращая ошибки нехватки памяти (`MemoryError`) на финальном этапе.
* **Настраиваемость:** Ключевые параметры, такие как размеры чанков и пакетов, вынесены в блок конфигурации для легкой настройки под конкретную систему.

## Требования

Для работы скрипта необходим Python 3.x и следующие библиотеки. Зависимости в requirements.txt. Установить их можно командой:

```bash
pip install pandas torch transformers sentencepiece sacremoses tqdm
```
**Важно:** Для работы на GPU у вас должна быть установлена видеокарта NVIDIA и соответствующая версия CUDA, совместимая с вашей версией PyTorch. Для поддержки CUDA нужно установить torch со следующими параметрами
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## Конфигурация

Все основные параметры находятся в начале скрипта в блоке настроек.

* `MODEL_NAME`: Название модели для перевода из репозитория Hugging Face. По умолчанию `Helsinki-NLP/opus-mt-ru-en`.
* `INPUT_FILE`: Имя исходного CSV-файла.
* `OUTPUT_FILE`: Имя файла, в который будет сохранен результат.
* `INPUT_ENCODING` / `OUTPUT_ENCODING`: Кодировки исходного и конечного файлов.
* `CHUNK_SIZE`: **Количество строк**, считываемых из исходного файла за один раз. Это ключевой параметр для балансировки нагрузки на **CPU и оперативную память (RAM)**.
* `BATCH_SIZE`: **Количество фраз**, которые одновременно обрабатываются на **GPU**. Это ключевой параметр для балансировки нагрузки на **видеопамять (VRAM)**.
* `LOG_FILE` / `LOG_LEVEL`: Настройки для файла логирования, куда записываются все шаги и ошибки.
* `USE_GPU`: `True` для использования GPU, `False` для принудительного использования CPU.

## Использование

1.  Настройте параметры в конфигурационном блоке скрипта.
2.  Поместите исходный `input.csv` в ту же папку.
3.  Запустите скрипт из командной строки:
    ```bash
    python run_translator.py
    ```
4.  Процесс выполнения будет отображаться с помощью прогресс-бара. По завершении в указанном `OUTPUT_FILE` появится переведенный файл.

## Как работает код

Процесс работы скрипта можно разбить на несколько ключевых этапов:

**1. Инициализация и загрузка модели (`main` и `load_model`)**
   - При запуске скрипт настраивает систему логирования.
   - Вызывается функция `load_model()`, которая определяет наличие GPU.
   - С помощью библиотеки `transformers` загружается указанная нейросетевая модель (`Helsinki-NLP/opus-mt-ru-en`) и ее токенизатор. Модель перемещается в память видеокарты.
   - Создается объект `pipeline` — это высокоуровневый интерфейс `transformers`, который инкапсулирует всю логику токенизации, перевода и постобработки.

**2. Чтение файла по частям (`main`)**
   - Основной цикл в функции `main` не загружает весь CSV-файл в память.
   - С помощью `itertools.islice` он эффективно считывает из файла порцию строк, равную `CHUNK_SIZE`. Это позволяет работать с файлами любого размера.
   - Каждая такая порция строк преобразуется в `pandas.DataFrame` для удобной дальнейшей обработки.

**3. Обработка одного чанка (`process_chunk`)**
   - Эта функция является ядром всей логики перевода.
   - **Шаг 3a: Извлечение и дедупликация фраз.**
     - Функция проходит по каждой строке в DataFrame.
     - С помощью регулярного выражения `RUSSIAN_PATTERN` она находит все фрагменты, содержащие кириллицу.
     - Все найденные фразы добавляются в объект `set`. `Set` — это структура данных в Python, которая автоматически хранит только **уникальные** значения. Это важнейшая оптимизация, которая предотвращает многократный перевод одного и того же слова (например, слова "ошибка").
   - **Шаг 3b: Пакетный перевод на GPU.**
     - Уникальные фразы из `set` передаются одним большим списком в объект `translator`.
     - Здесь происходит магия: `pipeline` разбивает этот список на мини-пакеты размером `BATCH_SIZE` и отправляет их на GPU. GPU, благодаря своей архитектуре, обрабатывает сотни фраз параллельно, что обеспечивает колоссальное ускорение.
     - При вызове переводчика используются специальные параметры (`max_length`, `no_repeat_ngram_size` и др.), чтобы предотвратить "глюки" модели, такие как зацикливание и генерация бессмысленного текста.
     - Результатом является словарь-карта вида `{'оригинал': 'перевод'}`.
   - **Шаг 3c: Безопасная замена текста.**
     - Для каждой исходной строки в чанке снова используется регулярное выражение `RUSSIAN_PATTERN`, но на этот раз с функцией `re.sub()`.
     - Для каждого найденного русского фрагмента `re.sub` вызывает callback-функцию `replace_callback`, которая смотрит в созданный ранее словарь и подставляет соответствующий перевод.
     - Этот подход гарантирует, что заменяются **только** русские слова, а весь окружающий код, знаки пунктуации и разметка остаются нетронутыми.
   - **Шаг 3d: Возврат результата.**
     - Функция возвращает `DataFrame` с уже переведенным текстом.

**4. Запись результата и повторение (`main`)**
   - Обработанный `DataFrame` записывается в выходной файл с помощью метода `.to_csv()`. Этот метод оптимизирован для работы с большими данными и не вызывает ошибок нехватки памяти, так как записывает данные на диск порциями, а не создает один гигантский объект в памяти.
   - Файл открывается в режиме дозаписи (`mode='a'`), поэтому каждый новый обработанный чанк добавляется в конец файла.
   - Прогресс-бар `tqdm` обновляется, и цикл повторяется для следующего чанка, пока весь исходный файл не будет прочитан.

---